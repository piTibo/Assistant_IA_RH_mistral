# Assistant_IA_RH_mistral
Assistant IA RH local ‚Äî bas√© sur le Code du travail 2025 üá´üá∑

Ce projet est un assistant IA local, con√ßu pour r√©pondre √† des questions li√©es au Code du travail fran√ßais, sans d√©pendance √† des services externes (fonctionne enti√®rement en local).  
Il s'appuie sur un moteur de recherche s√©mantique avec une base vectorielle Chroma, un mod√®le LLaMA (Mistral 7B) en .gguf ex√©cut√© via llama.cpp, et une interface utilisateur simple gr√¢ce √† Gradio.

Fonctionnalit√©s principales :
- Indexation automatique du Code du travail 2025 (PDF) en vecteurs.
- Recherche intelligente de passages pertinents √† l'aide de langchain et Chroma.
- G√©n√©ration de r√©ponses en langage naturel via un mod√®le local Mistral.
- Interface web minimaliste pour poser des questions.

Fiche Projet ‚Äî Assistant IA Juridique Local (Code du travail 2025)
Objectif

D√©velopper un assistant intelligent capable de r√©pondre √† des questions juridiques pr√©cises bas√©es sur le Code du travail 2025, en s‚Äôappuyant sur une approche Retrieval-Augmented Generation (RAG) pour citer pr√©cis√©ment les articles concern√©s.
Fonctionnalit√©s principales

    Recherche documentaire via une base vectorielle construite √† partir du texte du Code du travail.

    G√©n√©ration de r√©ponses contextualis√©es √† partir d‚Äôun mod√®le de langage local (Mistral 7B instruct quantifi√©).

    Citation des articles pr√©cis utilis√©s pour appuyer les r√©ponses.

    Interface utilisateur simple via Gradio pour poser les questions en langage naturel.

Technologies utilis√©es

    Mod√®le de langage : Mistral 7B instruct quantifi√© (mistral-7b-instruct-v0.1.Q4_K_M.gguf) charg√© via la biblioth√®que llama_cpp.

    Indexation et recherche :

        Embeddings g√©n√©r√©s avec sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 via langchain_huggingface.

        Vector store bas√© sur Chroma pour stocker et rechercher les passages pertinents.

    Cha√Æne RAG construite avec langchain RetrievalQA combinant le retriever Chroma et le mod√®le local.

    Interface Web : d√©velopp√©e avec Gradio pour une interaction facile en local.

Points techniques et d√©fis

    Gestion du contexte et limite de tokens (n_ctx=512 avec le mod√®le local).

    Mise en place d‚Äôune r√©cup√©ration efficace des documents pertinents (max 2 documents pour √©viter overflow).

    Am√©lioration de la g√©n√©ration pour √©viter les hallucinations et r√©ponses hors sujet (ex. g√©n√©rer plusieurs Q/R non demand√©es).

    Citation automatique des sources (articles du Code du travail) dans la r√©ponse.

    Int√©gration et quantification du mod√®le local pour performance GPU (RTX 3060 Ti).

Enjeux m√©tiers

    Fournir un assistant fiable et pr√©cis pour les questions juridiques, utile aux RH, juristes, et collaborateurs.

    Automatiser l‚Äôacc√®s aux textes complexes et volumineux (Code du travail).

    Am√©liorer l‚Äôexp√©rience utilisateur avec des r√©ponses argument√©es et sourc√©es.

Perspectives d‚Äô√©volution

    Optimiser le prompt et la g√©n√©ration pour des r√©ponses plus cibl√©es.

    Int√©grer la gestion des versions et mises √† jour du Code du travail.

    Ajouter des fonctionnalit√©s de synth√®se ou de comparaison entre articles.

    D√©ploiement possible sur serveur local s√©curis√© ou intranet d‚Äôentreprise.

## üìÅ Fichiers volumineux non inclus

Certaines ressources trop volumineuses ne sont **pas versionn√©es** dans le d√©p√¥t. Voici comment les reconstituer :

### üîπ Base de donn√©es Chroma

Le fichier `chroma_code_travail/chroma.sqlite3` est g√©n√©r√© automatiquement. Pour le reconstituer, il suffit de lancer :

```bash
python index_articles.py
```
### üîπ Mod√®le de langage LLaMA (Mistral)

Le fichier model/mistral-7b-instruct-v0.1.Q4_K_M.gguf peut √™tre t√©l√©charg√© depuis Hugging Face :

‚û°Ô∏è T√©l√©chargement sur HuggingFace

Mod√®le recommand√© : mistral-7b-instruct-v0.1.Q4_K_M.gguf

T√©l√©chargez-le et placez-le dans le dossier model/.


## Liste des d√©pendances
```bash
pip install langchain langchain-community langchain-huggingface llama-cpp-python gradio PyMuPDF
```
